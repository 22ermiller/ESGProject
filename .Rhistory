# Store samples after burn-in and according to thinning
if (i > burn_in && (i - burn_in) %% thin == 0) {
samples$beta[sample_count,] <- current$beta
samples$sigma[sample_count] <- current$sigma
samples$theta[sample_count,] <- current$theta
sample_count <- sample_count + 1
}
setTxtProgressBar(pb, i)
}
close(pb)
return(samples)
}
draws_fast <- MCMC_algorithm_thinned(y, m, V, a, 100000, 100)
draws_fast <- MCMC_algorithm_thinned(y, m, V, a, 10000, 10)
sigma_draws_fast <- draws_fast$sigma
beta_draws_fast <- draws_fast$beta
theta_draws_fast <- draws_fast$theta
plot(sigma_draws_fast, type = "l")
par(mfrow = c(2,3))
for (i in 1:6) {
plot(beta_draws_fast[,i], type = "l")
}
plot(sigma_draws_fast, type = "l")
runApp('Documents/Work/Executive Dashboard/BellaCapelliExecDash')
runApp('Documents/Work/Executive Dashboard/BellaCapelliExecDash')
runApp('Documents/Work/Executive Dashboard/BellaCapelliExecDash')
test_df <- sale_df |> filter(location == "Monroeville") |>
filter(SaleType == "Service") %>%
distinct(location, SaleDate, Client_TId) %>%
count(location, SaleDate) |> View()
test_df <- sale_df |> filter(location == "Monroeville") |>
filter(SaleType == "Service") %>% mutate(SaleDate = floor_date(SaleDate, unit = "month")) |>
distinct(location, SaleDate, Client_TId) %>%
count(location, SaleDate) |> View()
5+8+14+20+4+7+6+15+3+5+14+9+8+18+13+17
library(shiny)
runUrl("https://ajara.byu.edu/STAT651/Lectures/Lecture17.zip")
runApp('Documents/Work/Executive Dashboard/BellaCapelliExecDash')
runApp('Documents/Work/Executive Dashboard/BellaCapelliExecDash')
runApp('Documents/Work/Executive Dashboard/BellaCapelliExecDash')
shiny::runApp('Documents/Work/Executive Dashboard/BellaCapelliExecDash')
runApp('Documents/Work/Executive Dashboard/CincinnatiExecDashv3')
runApp('Documents/Work/Executive Dashboard/ExecDashv3')
runApp('Documents/Work/Executive Dashboard/KanskiExecDashv3')
runApp('Documents/Work/Executive Dashboard/TurnageExecDashv3')
# write the correct code
C <- matrix(c(rep(0,6),1,-1,0,0,0,0,
rep(0,6),1,0,-1,0,0,0,
rep(0,6),1,0,0,-1,0,0,
rep(0,6),1,0,0,0,-1,0,
rep(0,6),1,0,0,0,0,-1), nrow = 5, byrow = TRUE)
C
C2
# write the correct code
C2 <- matrix(c(1,-1,rep(0,10),
0,0,1,-1,rep(0,8),
0,0,0,0,1,-1,rep(0,6),
rep(0,6),1,-1,0,0,0,0,
rep(0,8),1,-1,0,0,
rep(0,10),1,-1), nrow = 6, byrow = TRUE)
C2
multcomp::glht(out, C2, alternative = "two.sided")
#| warning: false
library(tidyverse)
#| warning: false
ChildGrowth <- read_csv("https://grimshawville.byu.edu/ChildGrowth.csv")
ChildGrowth <- ChildGrowth |>
filter(Age < 15) |>
mutate(Gender = factor(Gender),
Ethnicity = factor(Ethnicity))
ChildGrowth |>
group_by(Ethnicity, Gender) |>
summary(n = count(),
Mean_Wt = mean(Weight), StDev_Wt = sd(Weight),
Min_Wt = min(Weight), P10_Wt = quartile(0.10, Weight),
P90_Wt = quartile(0.90, Weight), Max_Wt = max(Weight))
#| echo: false
#| warning: false
ggplot(ChildGrowth,
aes(x = Age,
y = log(Weight))) +
geom_point() +
geom_smooth() +
labs(
main = "Child Growth",
x = "Age (yrs)",
y = "log(Weight) (kg)"
)
ggplot(ChildGrowth,
aes(x = Age,
y = log(Weight),
col = Gender)) +
#geom_point() +
geom_smooth() +
facet_wrap(~ Ethnicity) +
labs(
main = "Child Growth",
x = "Age (yrs)",
y = "log(Weight) (kg)"
)
out <- lm(log(Weight) ~ 0 + (Gender : Ethnicity) + Gender : Ethnicity : Age, data = ChildGrowth, x = TRUE, y = TRUE)
summary(out)
# write the correct code
red.out <- lm(log(Weight) ~ 1 + Age, data = ChildGrowth, x = TRUE, y = TRUE)
summary(red.out)
anova(red.out, out)
# write the correct code
C <- matrix(c(rep(0,6),1,-1,0,0,0,0,
rep(0,6),1,0,-1,0,0,0,
rep(0,6),1,0,0,-1,0,0,
rep(0,6),1,0,0,0,-1,0,
rep(0,6),1,0,0,0,0,-1), nrow = 5, byrow = TRUE)
multcomp::glht(out, C, alternative = "two.sided")
multcomp::glht(out, C2, alternative = "two.sided")
test <- multcomp::glht(out, C2, alternative = "two.sided")
summary(test, test = Ftest())
summary(test, test = multcomp::Ftest())
# write the correct code
red.out <- lm(log(Weight) ~ 1 + Age, data = ChildGrowth, x = TRUE, y = TRUE)
summary(red.out)
head(red.out$x)
shiny::runApp('Documents/Work/Executive Dashboard/ExecDashv3')
runApp('Documents/Work/Executive Dashboard/ExecDashv3')
runApp('Documents/Work/Executive Dashboard/ExecDashv3')
runApp('Documents/Work/Executive Dashboard/TurnageExecDashv3')
runApp('Documents/Work/Executive Dashboard/TurnageExecDashv3')
runApp('Documents/Work/Executive Dashboard/AustinExecDashv3')
shiny::runApp('Documents/Work/Executive Dashboard/ExecDashv3')
library(httr)
library(httpuv)
library(curl)
library(jsonlite)
library(base64enc)
#Client ID and Client Secret were retrieved from the online explorer
clientID <- "ABkqwQRFXtqT9NY9UCJgjaHYoO3IVDwSW80MAqQNs9YJkXpS9E"
clientSecret <- "9kmfk5uFsS7jqyEamv9f1ujHZdx2wRxg741MC6T1"
scope <- "com.intuit.quickbooks.accounting"
RefreshToken <- "AB117082710904HTL2eLk1TF2v58G5mXHPm3naCAmqsx2X3Z8I"
authorize <- base64enc::base64encode(charToRaw(paste0(clientID,":",clientSecret)))
oauth_refresh <- httr::POST("https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer",
add_headers('Content-Type'= "application/x-www-form-urlencoded",
'Accept'= 'application/json',
'Authorization'= paste0('Basic ',authorize)
),
body = list('grant_type'='refresh_token',
'refresh_token'=RefreshToken),
encode = "form")
oaJSON <- fromJSON(content(oauth_refresh, as = "text"))
RefreshToken <- oaJSON[["refresh_token"]][1]
View(oauth_refresh)
authorize
View(oaJSON)
content(oauth_refresh, as = "text")
shiny::runApp('Documents/Work/Executive Dashboard/ExecDashv3')
runApp('Documents/Work/Executive Dashboard/ExecDashv3')
shiny::runApp('Documents/Work/Executive Dashboard/ExecDashv3')
runApp('Documents/Work/Executive Dashboard/ExecDashv3')
?input_switch
runApp('Documents/Work/Executive Dashboard/ExecDashv3')
getwd()
renv::status()
setwd("~/Documents/Work/GFF/MarginsProject")
renv::activate()
renv::activate("~/Documents/Work/GFF/MarginsProject")
library(gander)
gander_peek()
options(.gander_chat = ellmer::chat_github())
usethis::edit_r_profile()
library(gander)
gander_peek()
library(gander)
gander_peek()
library(ellmer)
gander_peek()
options(.gander_chat = ellmer::chat_github())
usethis::edit_r_profile()
library(gander)
gander_peek
gander_peek()
gander_peek()
gander_peek()
library(RMySQL)
library(tidyverse)
library(googlesheets4)
library(lubridate)
library(AzureStor)
library(data.table)
library(arrow)
mysqlconnection <- dbConnect(RMySQL::MySQL(),
dbname='crmtest',
host='crmdata-1.cluster-cuxuhbst9ezf.us-east-1.rds.amazonaws.com',
port=3306,
user='staritagroup',
password='D@T@Analytics!')
library(tidyverse)
library(vars)
library(forecast)
install.packages("colorspace")
install.packages("ellmer")
packageVersion("curl")
install.packages("ellmer", dependencies = TRUE)
setwd("~/Documents/GradSchool/AnnuityProject")
library(MSGARCH)
cpi_mod <- readRDS("models/cpi_mod.rds")
eci_mod <- readRDS("models/eci_mod.rds")
med_mod <- readRDS("models/med_mod.rds")
ir3mo_mod <- readRDS("models/interest_garch.rds")
yield_var_mod <- readRDS("models/yield_curve_var_mod.rds")
yield_lms <- readRDS("models/yield_curve_lm_mods.rds")
equity_mean_mod <- readRDS("models/equity_mean_mod.rds")
equity_mean_mod_resids <- readRDS("models/mean_mod_resids.rds")
# Read in functions
source("functions.R")
n_years <- 40
n_sims <- 100
cpi_df <- read_csv("data/cpi.csv") |>
filter(date >= "2010-01-01") |> # only dates from 2010 onwards
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi)) |>
filter(!is.na(log_dif_cpi))
library(rugarch)
library(dplyr)
# Read in functions
source("functions.R")
n_years <- 40
n_sims <- 100
cpi_df <- read_csv("data/cpi.csv") |>
filter(date >= "2010-01-01") |> # only dates from 2010 onwards
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi)) |>
filter(!is.na(log_dif_cpi))
library(tidyverse)
cpi_df <- read_csv("data/cpi.csv") |>
filter(date >= "2010-01-01") |> # only dates from 2010 onwards
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi)) |>
filter(!is.na(log_dif_cpi))
ir3mo_df <- read_csv("data/ir3mo.csv") |>
mutate(lagged_rate = lag(rate, n = 1),
rate_rmmean = rate - mean(rate, na.rm = TRUE)) |>
mutate(dif_rate = rate - lagged_rate,
log_dif_rate = log(rate) - log(lagged_rate))
full_ir_df <- read_csv("data/full_ir.csv") |>
mutate(across(three_month:thirty_year, ~./100)) |>
mutate(slope = thirty_year - three_month,
curve = three_month + thirty_year - (2*ten_year))
cpi_sim <- cpi_single_sim(cpi_mod, n_years*12)
library(forecast)
cpi_sim <- cpi_single_sim(cpi_mod, n_years*12)
cpi_sims <- cpi_multiple_sims(cpi_mod, n_years*12, n_sims)
final_cpi_val <- get_last_quarterly_cpi_val(cpi_df)
eci_sim <- eci_single_sim(eci_mod, n_years*4, cpi_sim = cpi_sim, final_cpi_value = final_cpi_val)
eci_sims <- eci_multiple_sims(eci_mod, n_years*4, n_sims, cpi_sims, final_cpi_val)
med_sim <- med_single_sim(med_mod, n_years*12)
med_sims <- med_multiple_sims(med_mod, n_years*12, n_sims)
mean_3mo_ir <- get_average_3mo_rate(ir3mo_df)
ir3mo_sim <- ir3mo_single_sim(ir3mo_mod, n_years*12, cpi_sim, mean_3mo_ir)
ir3mo_sims <- ir3mo_multiple_sims(ir3mo_mod, n_years*12, n_sims, cpi_sims, mean_3mo_ir)
slope_curve_vals <- get_final_slope_curve_vals(full_ir_df)
get_final_slope_curve_vals <- function(ir_df) {
list(slope = tail(ir_df$slope, 1),
curve = tail(ir_df$curve, 1))
}
slope_curve_vals <- get_final_slope_curve_vals(full_ir_df)
yield_curve_sim <- yield_single_sim(yield_var_mod, yield_lms, n_years*12, ir3mo_sim, slope_curve_vals)
library(tsDyn)
yield_curve_sim <- yield_single_sim(yield_var_mod, yield_lms, n_years*12, ir3mo_sim, slope_curve_vals)
equity_rs_mod <- fit_equity_rs_model(equity_mean_mod_resids)
equity_sim <- equity_single_sim(equity_mean_mod, equity_rs_mod, n_years*12, cpi_sim, ir3mo_sim)
equity_sims <- equity_multiple_sims(equity_mean_mod, equity_rs_mod, n_years*12, n_sims, cpi_sims, ir3mo_sims)
summary(cpi_mod)
# Libraries
library(tidyverse)
library(tidymodels)
library(forecast)
library(modeltime)
library(timetk)
cpi_df <- read_csv("data/cpi.csv") |>
filter(date >= "2010-01-01") |>
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi)) |>
filter(!is.na(log_dif_cpi)) # remove first observation without lag
med_inflation_df <- read_csv("data/med_inflation.csv") |>
filter(date >= min(cpi_df$date)) |> # only need data that cpi has
mutate(lagged_med = lag(med_inflation, n = 1)) |>
mutate(log_dif_med = log(med_inflation) - log(lagged_med)) |>
filter(!is.na(log_dif_med)) # remove first observation without lag
ggplot() +
#geom_line(aes(x = eci$date, y = eci$log_dif_eci, color = "ECI")) +
geom_line(aes(x = cpi_df$date, y = cpi_df$log_dif_cpi, color = "CPI")) +
geom_line(aes(x = med_inflation_df$date, y = med_inflation_df$log_dif_med, color = "Medical Inflation")) +
labs(
x = "Date",
y = "Value",
color = "Legend"
) +
scale_color_manual(values = c("CPI" = "black", "Medical Inflation" = "red"))
ggplot() +
geom_line(aes(x = eci_df$date, y = eci_df$log_dif_eci, color = "ECI")) +
labs(
title = "Log Difference of ECI",
x = "Date",
y = "Value",
color = "Legend"
) +
scale_color_manual(values = c("ECI" = "blue"))
cpi_df |> plot_time_series(date, log_dif_cpi)
install.packages("lazyeval")
cpi_df |> plot_time_series(date, log_dif_cpi)
install.packages("viridisLite")
cpi_df |> plot_time_series(date, log_dif_cpi)
install.packages("crosstalk")
cpi_df |> plot_time_series(date, log_dif_cpi)
cpi_df %>%
pull(log_dif_cpi) %>%
forecast::ggAcf(., lag.max = 2*365) +
ggtitle("CPI")
cpi_recipe <- recipe(log_dif_cpi~date, data = cpi_df) #%>%
ar_model <- arima_reg() |>
set_engine("auto_arima")
cpi_cv_split <- time_series_split(cpi_df, assess = '5 years', cumulative  = TRUE)
# Libraries
library(tidyverse)
library(tidymodels)
library(forecast)
library(modeltime)
library(timetk)
# CPI ---------------------------------------------------------------------
cpi_raw <- read_csv("data/cpi.csv")
cpi_df <- read_csv("data/cpi.csv") |>
filter(date >= "2010-01-01") |>
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi)) |>
filter(!is.na(log_dif_cpi)) # remove first observation without lag
# Ensure the data is ordered and converted into time series
log_dif_cpi_ts <- ts(cpi_df$log_dif_cpi, start = c(min(year(cpi_df$date)), month(min(cpi_df$date))), frequency = 12)
# Fit ARIMA(1,0,1) model
cpi_arima_model <- Arima(log_dif_cpi_ts, order = c(1, 0, 1))
# add diferencing
dif_arima_model <- Arima(log_dif_cpi_ts, order = c(0, 1, 2))
# optimal model from auto_arima
seasonal_arima <- Arima(log_dif_cpi_ts, order = c(1, 1, 1), seasonal = list(order = c(1, 0, 2), period = 12))
# Summary of the models
summary(cpi_arima_model)
#summary(dif_arima_model)
summary(seasonal_arima)
# AIC for each model
AIC(cpi_arima_model)
AIC(dif_arima_model)
AIC(seasonal_arima)
# Fit ARIMA(1,0,1) model
cpi_arima_model <- Arima(log_dif_cpi_ts, order = c(1, 0, 0))
# AIC for each model
AIC(cpi_arima_model)
quarterly_cpi <- cpi_df |>
filter(month(date) %in% c(3, 6, 9, 12)) |>
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi),
lagged_cpi = lag(log_dif_cpi, n = 1)) |>
filter(!is.na(log_dif_cpi) & !is.na(lagged_cpi)) # remove first observation without lag
eci_df <- read_csv("data/eci.csv") |>
filter(date >= min(quarterly_cpi$date)-months(3)) |> # only need data that cpi has
mutate(lagged_eci = lag(eci, n = 1)) |>
mutate(log_dif_eci = log(eci) - log(lagged_eci)) |>
filter(!is.na(log_dif_eci)) # remove first observation without lag
# Correlation between ECI and CPI
cor(eci_df$log_dif_eci, quarterly_cpi$log_dif_cpi)
# find correlation between ECI and CPI lagged by 1
cor(eci_df$log_dif_eci, quarterly_cpi$lagged_cpi, use = "complete.obs")
log_dif_eci_ts <- ts(eci_df$log_dif_eci, start = c(min(year(eci_df$date)), quarter(min(eci_df$date))), frequency = 4)
lagged_cpi_ts <- ts(quarterly_cpi$lagged_cpi, start = c(min(year(eci_df$date)), quarter(min(eci_df$date))), frequency = 4)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 1, 1), xreg = lagged_cpi_ts)
summary(arima_model_wlagcpi)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 0, 1), xreg = lagged_cpi_ts)
summary(arima_model_wlagcpi)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 1, 1), xreg = lagged_cpi_ts)
summary(arima_model_wlagcpi)
n_sims <- 1000
n_sims <- 200
sim_length <- 4*30
full_model_sims <- matrix(NA, nrow = n_sims, ncol = sim_length)
for(i in 1:n_sims){
log_dif_cpi_sim <- simulate(cpi_arima_model, nsim = sim_length*3)
# make simulations quarterly
quarterly_cpi_sim_df <- data.frame(
date = zoo::as.Date(time(log_dif_cpi_sim)),
value = log_dif_cpi_sim) |>
mutate(year = year(date),
quarter = quarter(date)) |>
group_by(year, quarter) |>
summarize(date = max(date),
log_dif_cpi = sum(value))
lagged_cpi_sim <- c(tail(quarterly_cpi$log_dif_cpi, n = 1), head(quarterly_cpi_sim_df$log_dif_cpi, n = -1))
full_model_sims[i,] <- simulate(arima_model_wlagcpi, nsim = sim_length, xreg = lagged_cpi_sim)
}
full_model_avg <- colMeans(full_model_sims)
full_model_ci <- apply(full_model_sims, 2, quantile, probs = c(.025, .975))
# plot simulation
plot(full_model_avg, type = "l", col = "red", ylim = c(-.008, .02))
lines(full_model_ci[2,], col = "red", lty = 2)
lines(full_model_ci[1,], col = "red", lty = 2)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 0, 1), xreg = lagged_cpi_ts)
n_sims <- 200
sim_length <- 4*30
full_model_sims <- matrix(NA, nrow = n_sims, ncol = sim_length)
for(i in 1:n_sims){
log_dif_cpi_sim <- simulate(cpi_arima_model, nsim = sim_length*3)
# make simulations quarterly
quarterly_cpi_sim_df <- data.frame(
date = zoo::as.Date(time(log_dif_cpi_sim)),
value = log_dif_cpi_sim) |>
mutate(year = year(date),
quarter = quarter(date)) |>
group_by(year, quarter) |>
summarize(date = max(date),
log_dif_cpi = sum(value))
lagged_cpi_sim <- c(tail(quarterly_cpi$log_dif_cpi, n = 1), head(quarterly_cpi_sim_df$log_dif_cpi, n = -1))
full_model_sims[i,] <- simulate(arima_model_wlagcpi, nsim = sim_length, xreg = lagged_cpi_sim)
}
full_model_avg <- colMeans(full_model_sims)
full_model_ci <- apply(full_model_sims, 2, quantile, probs = c(.025, .975))
# plot simulation
plot(full_model_avg, type = "l", col = "red", ylim = c(-.008, .02))
lines(full_model_ci[2,], col = "red", lty = 2)
lines(full_model_ci[1,], col = "red", lty = 2)
test_ts <- diff(log_dif_eci_ts)
arima_model_wlagcpi <- Arima(test_ts, order = c(1, 0, 1), xreg = lagged_cpi_ts)
arima_model_wlagcpi <- Arima(test_ts, order = c(1, 0, 1), xreg = lagged_cpi_ts[-1])
summary(arima_model_wlagcpi)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 1, 1), xreg = lagged_cpi_ts)
summary(arima_model_wlagcpi)
head(time(test_ts))
head(time(lagged_cpi_ts[-1]))
head(time(lagged_cpi_ts))
test_ts <- diff(log_dif_eci_ts)
head(time(test_ts))
arima_model_wlagcpi$arma
arima_model_wlagcpi$call
arima_model_wlagcpi$mask
arima_model_wlagcpi$fitted
cpi_df <- read_csv("data/cpi.csv") |>
filter(date >= "1990-01-01") |> # only dates from 2010 onwards
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi)) |>
filter(!is.na(log_dif_cpi)) # remove first observation without lag
# Ensure the data is ordered and converted into time series
log_dif_cpi_ts <- ts(cpi_df$log_dif_cpi, start = c(min(year(cpi_df$date)), month(min(cpi_df$date))), frequency = 12)
# Fit ARIMA(1,0,1) model
cpi_arima_model <- Arima(log_dif_cpi_ts, order = c(1, 0, 1))
summary(cpi_arima_model)
saveRDS(cpi_arima_model, "models/cpi_mod.rds")
quarterly_cpi <- cpi_df |>
filter(month(date) %in% c(3, 6, 9, 12)) |>
mutate(lagged_cpi = lag(cpi, n = 1)) |>
mutate(log_dif_cpi = log(cpi) - log(lagged_cpi),
lagged_cpi = lag(log_dif_cpi, n = 1)) |>
filter(!is.na(log_dif_cpi) & !is.na(lagged_cpi)) # remove first observation without lag
eci_df <- read_csv("data/eci.csv") |>
filter(date >= min(quarterly_cpi$date)-months(3)) |> # only need data that cpi has
mutate(lagged_eci = lag(eci, n = 1)) |>
mutate(log_dif_eci = log(eci) - log(lagged_eci)) |>
filter(!is.na(log_dif_eci)) # remove first observation without lag
# Correlation between ECI and CPI
cor(eci_df$log_dif_eci, quarterly_cpi$log_dif_cpi)
# find correlation between ECI and CPI lagged by 1
cor(eci_df$log_dif_eci, quarterly_cpi$lagged_cpi, use = "complete.obs")
log_dif_eci_ts <- ts(eci_df$log_dif_eci, start = c(min(year(eci_df$date)), quarter(min(eci_df$date))), frequency = 4)
lagged_cpi_ts <- ts(quarterly_cpi$lagged_cpi, start = c(min(year(eci_df$date)), quarter(min(eci_df$date))), frequency = 4)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 0, 1), xreg = lagged_cpi_ts)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 1, 1), xreg = lagged_cpi_ts)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 0, 1), xreg = lagged_cpi_ts)
summary(arima_model_wlagcpi)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 1, 1), xreg = lagged_cpi_ts)
summary(arima_model_wlagcpi)
arima_model_wlagcpi <- Arima(log_dif_eci_ts, order = c(1, 0, 1), xreg = lagged_cpi_ts)
n_sims <- 200
sim_length <- 4*30
full_model_sims <- matrix(NA, nrow = n_sims, ncol = sim_length)
for(i in 1:n_sims){
log_dif_cpi_sim <- simulate(cpi_arima_model, nsim = sim_length*3)
# make simulations quarterly
quarterly_cpi_sim_df <- data.frame(
date = zoo::as.Date(time(log_dif_cpi_sim)),
value = log_dif_cpi_sim) |>
mutate(year = year(date),
quarter = quarter(date)) |>
group_by(year, quarter) |>
summarize(date = max(date),
log_dif_cpi = sum(value))
lagged_cpi_sim <- c(tail(quarterly_cpi$log_dif_cpi, n = 1), head(quarterly_cpi_sim_df$log_dif_cpi, n = -1))
full_model_sims[i,] <- simulate(arima_model_wlagcpi, nsim = sim_length, xreg = lagged_cpi_sim)
}
full_model_avg <- colMeans(full_model_sims)
full_model_ci <- apply(full_model_sims, 2, quantile, probs = c(.025, .975))
# plot simulation
plot(full_model_avg, type = "l", col = "red", ylim = c(-.008, .02))
lines(full_model_ci[2,], col = "red", lty = 2)
lines(full_model_ci[1,], col = "red", lty = 2)
# Correlation between ECI and CPI
cor(eci_df$log_dif_eci, quarterly_cpi$log_dif_cpi)
# find correlation between ECI and CPI lagged by 1
cor(eci_df$log_dif_eci, quarterly_cpi$lagged_cpi, use = "complete.obs")
test_ts <- diff(log_dif_eci_ts)
log_dif_eci_ts <- ts(eci_df$log_dif_eci, start = c(min(year(eci_df$date)), quarter(min(eci_df$date))), frequency = 4)
lagged_cpi_ts <- ts(quarterly_cpi$lagged_cpi, start = c(min(year(eci_df$date)), quarter(min(eci_df$date))), frequency = 4)
test_ts <- diff(log_dif_eci_ts)
summary(arima_model_wlagcpi)
plot(log_dif_eci_ts)
cor(log_dif_eci_ts, lagged_cpi_ts)
plot(log_dif_eci_ts, lagged_cpi_ts)
library(tseries)
adf.test(log_dif_eci_ts)
